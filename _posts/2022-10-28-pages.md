---
layout: post
title: Dasom Ahn (Tommy Ahn) 
---

<p style='margin: 0.3in;'>
 I am a master's student in the Department of Computer Science and Engineering at Keimyung University.
 I am interested in computer vision and deep learning and doing research. My current research is focused on:
</p>
  - Computation-efficient architectures that enable efficient and generic modeling for activity recognition

### News

* [2023/01] <a href='https://arxiv.org/abs/2210.07503'>STAR-Transformer</a> is accepted to <a href='https://wacv2023.thecvf.com/'>WACV 2023</a>.
* [2022/07] <a href='https://ieeexplore.ieee.org/document/9895100'>Shift-ViT</a> is accepted to <a href='https://www.itc-cscc2022.org/'>ITC-CSCC 2022</a>.


### Publications
(* indicates equal contribution)


<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
 <tbody>
    <tr>
      <td style="padding:20px;width:30%;max-width:30%" align="center">
        <img style="width:100%;max-width:100%" src="../img/star.png" alt="dise">
      </td>
      <td width="75%" valign="center">
        <papertitle>STAR-Transformer: A Spatio-temporal Cross Attention Transformer for Human Action Recognition
        <br>
        <strong>Dasom Ahn</strong>, 
        <a href="https://jumpsnack.github.io/"> Sangwon Kim</a>, Hyunsu Hong, Byoung Chul Ko
        <br>
        <em>Conference on IEEE/CVF Winter Conference on Applications of Computer Vision (<strong>WACV</strong>)</em>, 2023
        <br>
        <a href="https://arxiv.org/abs/2210.07503">[arXiv]</a>
        <br>
        <p> In action recognition, although the combination of spatio-temporal videos and skeleton features can improve the recognition performance, a separate model and balancing feature representation for cross-modal data are required. To solve these problems, we propose Spatio-TemporAl cRoss (STAR)-transformer, which can effectively represent two cross-modal features as a recognizable vector.</p>
      </td>
    </tr>

  </tbody>
</table>
