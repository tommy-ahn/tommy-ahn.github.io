---
layout: post
title: Dasom Ahn (Tommy Ahn) 
---

<p style='margin: 0.3in;'>
 I am a master's student in the Department of Computer Science and Engineering at Keimyung University.
 I am interested in computer vision and deep learning and doing research. My current research is focused on:
</p>
  - Computation-efficient architectures that enable efficient and generic modeling for activity recognition

### News

* [2023/01] <a href='https://arxiv.org/abs/2210.07503'>STAR-Transformer</a> is accepted to <a href='https://wacv2023.thecvf.com/'>WACV 2023</a>.
* [2022/07] <a href='https://ieeexplore.ieee.org/document/9895100'>Shift-ViT</a> is accepted to <a href='https://www.itc-cscc2022.org/'>ITC-CSCC 2022</a>.


### Publications
(* indicates equal contribution)


<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;font-size:12pt;">
 <tbody>
    <tr>
      <td style="padding:20px;width:30%;max-width:30%" align="center">
        <img style="width:100%;max-width:100%" src="../img/star.png" alt="dise">
      </td>
      <td width="75%" valign="center">
        <papertitle>STAR-Transformer: A Spatio-temporal Cross Attention Transformer for Human Action Recognition
        <br>
        <strong>Dasom Ahn</strong>, 
        <a href="https://jumpsnack.github.io/"> Sangwon Kim</a>, Hyunsu Hong, Byoung Chul Ko
        <br>
        <em>Conference on IEEE/CVF Winter Conference on Applications of Computer Vision (<strong>WACV</strong>)</em>, 2023
        <br>
        <a href="https://arxiv.org/abs/2210.07503">[arXiv]</a>
        <br>
        <p style="font-size:9pt;"> In action recognition, although the combination of spatio-temporal videos and skeleton features can improve the recognition performance, a separate model and balancing feature representation for cross-modal data are required. To solve these problems, we propose Spatio-TemporAl cRoss (STAR)-transformer, which can effectively represent two cross-modal features as a recognizable vector.</p>
    
      
    <tr>
      <td style="padding:20px;width:30%;max-width:30%" align="center">
        <img style="width:100%;max-width:100%" src="../img/shift.png" alt="dise">
      </td>
      <td width="75%" valign="center">
        <papertitle>Shift-ViT: Siamese Vision Transformer using Shifted Branches
        <br>
        <strong>Dasom Ahn</strong>, 
        Hyeongjin Kim, <a href="https://jumpsnack.github.io/"> Sangwon Kim</a>, Hyunsu Hong, Byoung Chul Ko
        <br>
        <em>International Technical Conference o Circuits/Systems, Computers and Communications (<strong>ITC-CSCC</strong>)</em>, 2022
        <br>
        <a href="https://ieeexplore.ieee.org/document/9895100">[IEEE Xplore]</a>
        <br>
        <p style="font-size:9pt;"> In this paper, we propose a novel classification method to overcome the limitation of Vision Transformer (ViT).  ViT has no inductive bias fundamentally unlike conventional Convolution Neural Network (CNN) models. For that reason, previous ViTs are limited to handle sensitively to small changes in the image. Therefore, in this paper, we propose Shift-ViT, a new algorithm that adaptively learns image changes by extending the conventional ViT. </p>
    
